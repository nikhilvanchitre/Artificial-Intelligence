{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network using just NumPy\n",
    "\n",
    "2 layer NN with EXOR output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "n_hidden = 10\n",
    "n_in = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs\n",
    "n_out = 10\n",
    "#sample data\n",
    "n_sample = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non deterministic seeding\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function(turn the number into a probability)\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function________inputdata,transpose, layer1, layer2, biases\n",
    "def train(x, t, V, W, bv, bw):\n",
    "    #forward -- matrix mult + biases\n",
    "    A = np.dot(x,V)+ bv\n",
    "    Z = np.tanh(A)\n",
    "    \n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "    \n",
    "    #backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "    \n",
    "    #predict loss\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "    \n",
    "    #cross entropy (Classification special)\n",
    "    loss = -np.mean( t * np.log(Y) + (1 - t) * np.log(1-Y))\n",
    "    \n",
    "    return loss, (dV, dW, Ev, Ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.68475420, Time: 0.0032s\n",
      "Epoch: 1, Loss: 0.64745443, Time: 0.0034s\n",
      "Epoch: 2, Loss: 0.60091689, Time: 0.0033s\n",
      "Epoch: 3, Loss: 0.52489902, Time: 0.0039s\n",
      "Epoch: 4, Loss: 0.44424946, Time: 0.0038s\n",
      "Epoch: 5, Loss: 0.37638602, Time: 0.0036s\n",
      "Epoch: 6, Loss: 0.32319174, Time: 0.0027s\n",
      "Epoch: 7, Loss: 0.28330851, Time: 0.0030s\n",
      "Epoch: 8, Loss: 0.25325634, Time: 0.0017s\n",
      "Epoch: 9, Loss: 0.22955980, Time: 0.0016s\n",
      "Epoch: 10, Loss: 0.20995226, Time: 0.0016s\n",
      "Epoch: 11, Loss: 0.19317745, Time: 0.0016s\n",
      "Epoch: 12, Loss: 0.17858330, Time: 0.0016s\n",
      "Epoch: 13, Loss: 0.16581988, Time: 0.0017s\n",
      "Epoch: 14, Loss: 0.15464618, Time: 0.0017s\n",
      "Epoch: 15, Loss: 0.14484251, Time: 0.0018s\n",
      "Epoch: 16, Loss: 0.13619547, Time: 0.0017s\n",
      "Epoch: 17, Loss: 0.12850917, Time: 0.0016s\n",
      "Epoch: 18, Loss: 0.12161565, Time: 0.0016s\n",
      "Epoch: 19, Loss: 0.11537809, Time: 0.0016s\n",
      "Epoch: 20, Loss: 0.10968842, Time: 0.0016s\n",
      "Epoch: 21, Loss: 0.10446278, Time: 0.0017s\n",
      "Epoch: 22, Loss: 0.09963652, Time: 0.0021s\n",
      "Epoch: 23, Loss: 0.09515969, Time: 0.0021s\n",
      "Epoch: 24, Loss: 0.09099331, Time: 0.0016s\n",
      "Epoch: 25, Loss: 0.08710647, Time: 0.0016s\n",
      "Epoch: 26, Loss: 0.08347403, Time: 0.0016s\n",
      "Epoch: 27, Loss: 0.08007499, Time: 0.0016s\n",
      "Epoch: 28, Loss: 0.07689128, Time: 0.0017s\n",
      "Epoch: 29, Loss: 0.07390698, Time: 0.0027s\n",
      "Epoch: 30, Loss: 0.07110775, Time: 0.0026s\n",
      "Epoch: 31, Loss: 0.06848043, Time: 0.0024s\n",
      "Epoch: 32, Loss: 0.06601288, Time: 0.0023s\n",
      "Epoch: 33, Loss: 0.06369378, Time: 0.0018s\n",
      "Epoch: 34, Loss: 0.06151258, Time: 0.0018s\n",
      "Epoch: 35, Loss: 0.05945946, Time: 0.0021s\n",
      "Epoch: 36, Loss: 0.05752524, Time: 0.0019s\n",
      "Epoch: 37, Loss: 0.05570143, Time: 0.0027s\n",
      "Epoch: 38, Loss: 0.05398010, Time: 0.0032s\n",
      "Epoch: 39, Loss: 0.05235395, Time: 0.0016s\n",
      "Epoch: 40, Loss: 0.05081621, Time: 0.0017s\n",
      "Epoch: 41, Loss: 0.04936063, Time: 0.0033s\n",
      "Epoch: 42, Loss: 0.04798147, Time: 0.0016s\n",
      "Epoch: 43, Loss: 0.04667342, Time: 0.0016s\n",
      "Epoch: 44, Loss: 0.04543161, Time: 0.0027s\n",
      "Epoch: 45, Loss: 0.04425153, Time: 0.0022s\n",
      "Epoch: 46, Loss: 0.04312906, Time: 0.0018s\n",
      "Epoch: 47, Loss: 0.04206039, Time: 0.0019s\n",
      "Epoch: 48, Loss: 0.04104200, Time: 0.0036s\n",
      "Epoch: 49, Loss: 0.04007068, Time: 0.0020s\n",
      "Epoch: 50, Loss: 0.03914344, Time: 0.0019s\n",
      "Epoch: 51, Loss: 0.03825752, Time: 0.0029s\n",
      "Epoch: 52, Loss: 0.03741040, Time: 0.0020s\n",
      "Epoch: 53, Loss: 0.03659972, Time: 0.0019s\n",
      "Epoch: 54, Loss: 0.03582330, Time: 0.0019s\n",
      "Epoch: 55, Loss: 0.03507914, Time: 0.0024s\n",
      "Epoch: 56, Loss: 0.03436537, Time: 0.0019s\n",
      "Epoch: 57, Loss: 0.03368025, Time: 0.0018s\n",
      "Epoch: 58, Loss: 0.03302218, Time: 0.0025s\n",
      "Epoch: 59, Loss: 0.03238966, Time: 0.0019s\n",
      "Epoch: 60, Loss: 0.03178128, Time: 0.0018s\n",
      "Epoch: 61, Loss: 0.03119576, Time: 0.0019s\n",
      "Epoch: 62, Loss: 0.03063187, Time: 0.0024s\n",
      "Epoch: 63, Loss: 0.03008849, Time: 0.0018s\n",
      "Epoch: 64, Loss: 0.02956454, Time: 0.0019s\n",
      "Epoch: 65, Loss: 0.02905905, Time: 0.0019s\n",
      "Epoch: 66, Loss: 0.02857107, Time: 0.0025s\n",
      "Epoch: 67, Loss: 0.02809974, Time: 0.0018s\n",
      "Epoch: 68, Loss: 0.02764424, Time: 0.0018s\n",
      "Epoch: 69, Loss: 0.02720381, Time: 0.0022s\n",
      "Epoch: 70, Loss: 0.02677771, Time: 0.0019s\n",
      "Epoch: 71, Loss: 0.02636528, Time: 0.0018s\n",
      "Epoch: 72, Loss: 0.02596588, Time: 0.0019s\n",
      "Epoch: 73, Loss: 0.02557890, Time: 0.0019s\n",
      "Epoch: 74, Loss: 0.02520377, Time: 0.0031s\n",
      "Epoch: 75, Loss: 0.02483998, Time: 0.0019s\n",
      "Epoch: 76, Loss: 0.02448700, Time: 0.0021s\n",
      "Epoch: 77, Loss: 0.02414437, Time: 0.0021s\n",
      "Epoch: 78, Loss: 0.02381164, Time: 0.0018s\n",
      "Epoch: 79, Loss: 0.02348838, Time: 0.0020s\n",
      "Epoch: 80, Loss: 0.02317420, Time: 0.0019s\n",
      "Epoch: 81, Loss: 0.02286871, Time: 0.0027s\n",
      "Epoch: 82, Loss: 0.02257156, Time: 0.0021s\n",
      "Epoch: 83, Loss: 0.02228240, Time: 0.0021s\n",
      "Epoch: 84, Loss: 0.02200091, Time: 0.0017s\n",
      "Epoch: 85, Loss: 0.02172679, Time: 0.0019s\n",
      "Epoch: 86, Loss: 0.02145975, Time: 0.0018s\n",
      "Epoch: 87, Loss: 0.02119950, Time: 0.0021s\n",
      "Epoch: 88, Loss: 0.02094580, Time: 0.0034s\n",
      "Epoch: 89, Loss: 0.02069839, Time: 0.0046s\n",
      "Epoch: 90, Loss: 0.02045703, Time: 0.0035s\n",
      "Epoch: 91, Loss: 0.02022151, Time: 0.0034s\n",
      "Epoch: 92, Loss: 0.01999160, Time: 0.0037s\n",
      "Epoch: 93, Loss: 0.01976710, Time: 0.0057s\n",
      "Epoch: 94, Loss: 0.01954782, Time: 0.0022s\n",
      "Epoch: 95, Loss: 0.01933358, Time: 0.0019s\n",
      "Epoch: 96, Loss: 0.01912419, Time: 0.0019s\n",
      "Epoch: 97, Loss: 0.01891949, Time: 0.0020s\n",
      "Epoch: 98, Loss: 0.01871932, Time: 0.0020s\n",
      "Epoch: 99, Loss: 0.01852352, Time: 0.0052s\n",
      "XOR prediction:\n",
      "[1 1 1 1 1 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "\n",
    "\n",
    "\n",
    "bv = np.zeros(n_hidden)\n",
    "\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "\n",
    "\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "\n",
    "\n",
    "# Generate some data\n",
    "\n",
    "\n",
    "\n",
    "X = np.random.binomial(1, 0.5, (n_sample, n_in))\n",
    "\n",
    "T = X ^ 1\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    err = []\n",
    "\n",
    "    upd = [0]*len(params)\n",
    "\n",
    "\n",
    "\n",
    "    t0 = time.clock()\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "\n",
    "        loss, grad = train(X[i], T[i], *params)\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(len(params)):\n",
    "\n",
    "            params[j] -= upd[j]\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(len(params)):\n",
    "\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "\n",
    "\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "\n",
    "\n",
    "    print('Epoch: %d, Loss: %.8f, Time: %.4fs' % (epoch, np.mean( err ), time.clock()-t0 ))\n",
    "\n",
    "\n",
    "\n",
    "# Try to predict something\n",
    "\n",
    "\n",
    "\n",
    "x = np.random.binomial(1, 0.5, n_in)\n",
    "\n",
    "print(\"XOR prediction:\")\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(predict(x, *params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
